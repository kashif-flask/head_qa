{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d84f71",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "479321b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_en = load_dataset('head_qa', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6132595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=data_en[\"train\"]\n",
    "val_data=data_en[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0994cd",
   "metadata": {},
   "source": [
    "## Fine tuning GPT3 on topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991dbc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics=[\"biology\",\"nursery\",\"psychology\",\"chemistry\",\"pharmacology\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dct={}\n",
    "qa_dct_val={}\n",
    "def all_topics(topic):\n",
    "    def topic_qa(example):\n",
    "        return example[\"category\"]==topic\n",
    "    \n",
    "    data_train_topic=train_data.filter(topic_qa)\n",
    "    data_val_topic=val_data.filter(topic_qa)\n",
    "    print(f\"(Topic-{topic.capitalize()})\")\n",
    "\n",
    "    \n",
    "    for example in data_train_topic:\n",
    "        answer=[x[\"atext\"] for x in example[\"answers\"] if x[\"aid\"]==example[\"ra\"]][0]\n",
    "        k=0\n",
    "        for e in example[\"answers\"]:\n",
    "            if e[\"aid\"]==example[\"ra\"]:\n",
    "                correct_char=chr(65+k)\n",
    "            k+=1\n",
    "        ques=topic.capitalize()+\":\\n\"+example[\"qtext\"]\n",
    "        qa_dct[ques]=\"\\n\".join([f\"{chr(65+i)}) \"+x[\"atext\"] for i,x in enumerate(example[\"answers\"])])+f\"\\n Correct:{correct_char}) \"+answer\n",
    "        \n",
    "    \n",
    "    for example in data_val_topic:\n",
    "        answer=[x[\"atext\"] for x in example[\"answers\"] if x[\"aid\"]==example[\"ra\"]][0]\n",
    "        k=0\n",
    "        for e in example[\"answers\"]:\n",
    "            if e[\"aid\"]==example[\"ra\"]:\n",
    "                correct_char=chr(65+k)\n",
    "            k+=1\n",
    "        ques=topic.capitalize()+\":\\n\"+example[\"qtext\"]\n",
    "        qa_dct_val[ques]=\"\\n\".join([f\"{chr(65+i)}) \"+x[\"atext\"] for i,x in enumerate(example[\"answers\"])])+f\"\\n Correct:{correct_char}) \"+answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48bd5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Topic-Biology)\n",
      "(Topic-Nursery)\n",
      "(Topic-Psychology)\n",
      "(Topic-Chemistry)\n",
      "(Topic-Pharmacology)\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    all_topics(topic)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "items=list(qa_dct.items())\n",
    "random.shuffle(items)\n",
    "qa_dct=dict(items)\n",
    "\n",
    "items=list(qa_dct_val.items())\n",
    "random.shuffle(items)\n",
    "qa_dct_val=dict(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c1ee13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def build_text_files(qa_dct,dest_path):\n",
    "    f = open(dest_path, 'w',encoding=\"utf-8\")\n",
    "    data = ''\n",
    "    for key,value in qa_dct.items():\n",
    "\n",
    "        summary =key+\"\\n\"+value\n",
    "    \n",
    "        data += summary + \"  \"+\"\\n\\n\"\n",
    "    data=data.encode(\"utf-8\").decode(\"utf-8\")\n",
    "    f.write(data)\n",
    "\n",
    "\n",
    "\n",
    "build_text_files(qa_dct,'train_dataset.txt')\n",
    "build_text_files(qa_dct_val,'test_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a809f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 13:26:44.429621: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-06 13:26:44.473325: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-06 13:26:45.417280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "train_path=f'train_dataset.txt'\n",
    "test_path=f'test_dataset.txt'\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=180)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=180)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f010256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./gpt-all\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=10, # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    per_device_eval_batch_size=8,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=8000, # after # steps model is saved\n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "084bdc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3050' max='3050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3050/3050 11:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.283200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.781700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.640800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3050, training_loss=1.236273718036589, metrics={'train_runtime': 679.0518, 'train_samples_per_second': 17.952, 'train_steps_per_second': 4.492, 'total_flos': 1119415259750400.0, 'train_loss': 1.236273718036589, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72/72 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.5818235874176025,\n",
       " 'eval_runtime': 8.6375,\n",
       " 'eval_samples_per_second': 65.875,\n",
       " 'eval_steps_per_second': 8.336,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0eecf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "863ee920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "\n",
    "model_trained = AutoModelForCausalLM.from_pretrained(f\"./gpt-all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42c4fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator=pipeline('text-generation',model=model_trained, tokenizer=tokenizer,\n",
    "                   device=-1,  # GPU index to use, -1 for CPU\n",
    "   \n",
    "                    max_length=148,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=0,\n",
    "                    num_return_sequences=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic=\"chemistry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25b8cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt=f\"{topic.capitalize()}:\"\n",
    "result=generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbecf0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chemistry:\n",
      "Vasioligo is an acid:\n",
      "A) To the iodo. acid.\n",
      "B) To the iodine.\n",
      "C) To the iodide.\n",
      "D) With the chloride.\n",
      "E) With the iodide.\n",
      "  Correct :D) With the chloride.\n"
     ]
    }
   ],
   "source": [
    "print(result[0][\"generated_text\"].split(\"Correct\")[0],\"Correct\",result[0][\"generated_text\"].split(\"Correct\")[1].split(\"\\n\")[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=\"your_token\"\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde20ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_trained.push_to_hub(\"gpt_head_qa\")\n",
    "tokenizer.push_to_hub(\"gpt_head_qa\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
